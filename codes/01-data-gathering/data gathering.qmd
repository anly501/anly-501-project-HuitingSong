---
title: "Data Gathering"
format: 
    html: 
        code-fold: true
        self-contained: true
jupyter: python3
---

# Data Gathering

I used twitter API to scratch mutiple keywords:**"energy" "energy economy" "consumption" "data" "USA"** to gather comments from twiter about their opinion about the energy consumption and energy economy. I use these data to analyze the positive and negative opinion. I can detect the internet users' attitudes to energy influence on economy. I can also use this datasets to define the relationship between economy and data. The keywords may not clear enough for me to analysis but I will adjust it later for future research. I used for loop to search over 600 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. I plan to detect the frequency of words to gain a plot. More than this, I plan to use Naive Bytes to give each tweet a positive or negative attitude.

## Twitter API in Python
The python package we will use in gathering data.
```{python}
import json 
import tweepy
import requests
from pandas import json_normalize 
import pandas as pd
```

Using the keys from Twitter and defined key words to scratch data into json file
```{python}
# READ FILE
f = open("/Users/apple/Desktop/anly-501-project-HuitingSong-1/codes/01-data-gathering/api-keys.json")
input=json.load(f); #print(input)

# LOAD KEYS INTO API
consumer_key=input["consumer_key"]    
consumer_secret=input["consumer_secret"]    
access_token=input["access_token"]    
access_token_secret=input["access_token_secret"]    
bearer_token=input["bearer_token"]

# Set up Connection
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)

# Add the search_twitter function here.
def search_twitter(query, max_results,tweet_fields, bearer_token = bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    url = "https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&{}".format(query, max_results,tweet_fields)
    print("--------------",url,"--------------")
    response = requests.request("GET", url, headers=headers)
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

tweet_fields = "tweet.fields=text,author_id,created_at,lang"

data = "/Users/apple/Desktop/anly-501-project-HuitingSong-1/codes/01-data-gathering/"
search_tweets = ["energy","energy economy","consumption","data","USA"]
for idx,val in enumerate(search_tweets):
    tweets_jsondump = []
    json_response1 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response2 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response3 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response4 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response5 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response6 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    for i in json_response1['data']:
        tweets_jsondump.append(i)
    for i in json_response2['data']:
        tweets_jsondump.append(i)
    for i in json_response3['data']:
        tweets_jsondump.append(i)
    for i in json_response4['data']:
        tweets_jsondump.append(i)
    for i in json_response5['data']:
        tweets_jsondump.append(i)
    for i in json_response6['data']:
        tweets_jsondump.append(i)
    with open(data+str(val)+'.json','w') as json_file:
        json.dump(tweets_jsondump,json_file)
        json_file.close()
```

Normalize the json file and form a data frame 
```{python}
twitterdf1 = json_normalize(json_response1,"data")
twitterdf2 = json_normalize(json_response2,"data")
twitterdf3 = json_normalize(json_response3,"data")
twitterdf4 = json_normalize(json_response4,"data")
twitterdf5 = json_normalize(json_response5,"data")
twitterdf6 = json_normalize(json_response6,"data")
twitterdf = [twitterdf1,twitterdf2,twitterdf3,twitterdf4,twitterdf5,twitterdf6]
twitterdf = pd.concat(twitterdf)
twitterdf
```

Save the data frame as csv file for futhur using
```{python}
twitterdf.to_csv("/Users/apple/Desktop/anly-501-project-HuitingSong-1/codes/01-data-gathering/twitterpython.csv")
```

## Twitter API in R

```{r message=FALSE, warning=FALSE, include=FALSE}
library(selectr)
library(rvest)
library(xml2)
library(rtweet) # for scraping tweets
library(wordcloud2) # for generating really cool looking wordclouds
library(tm) # for text minning
library(dplyr) # loads of fun stuff including piping
library(ROAuth)
library(jsonlite)
library(httpuv)
library(twitteR)
```

```{r}
consumer_key = "zV2ER7zM1HzFUshDEHpJ8UuVS"
consumer_secret = "5uHStDPwbFxEfLZojqPP9eDcWWXgfNCwdVPe8VG0xFvPldjhHJ"
access_token = "1558952162692108289-bQDaHCqBuJZnXXqeQWnz7Pk1bhdiWm"
access_token_secret = "kSJLq0tng5MfaFHoxbc7GpQ6D4SaplXzthDrW2SzhTSpD"
bearer_token = "AAAAAAAAAAAAAAAAAAAAAEGthAEAAAAArntLVGGx5Q8Irzy%2BRTc2HdgQ7Vw%3DOR64kNsk2o0eqNl9KKZQKs7i2yzUoHEObJHwsQWiwesFtlQKdp"

requestURL='https://api.twitter.com/oauth/request_token'
accessURL='https://api.twitter.com/oauth/access_token'
authURL='https://api.twitter.com/oauth/authorize'

s_key  = 'natural gas OR gasoline OR crude oil'
n_tweets = 250 

twitteR:::setup_twitter_oauth(consumer_key, consumer_secret,access_token,access_token_secret)
Search1<-twitteR::searchTwitter(s_key,n=n_tweets, since="2022-05-01",lang="en")
```

```{r}
TweetsDF<- twListToDF(Search1)
TweetsDF
write.csv(TweetsDF,"/Users/apple/Desktop/anly-501-project-HuitingSong-1/codes/01-data-gathering/twitterR.csv")
```