{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Data Gathering\n",
        "format:\n",
        "  html:\n",
        "    code-fold: true\n",
        "    self-contained: true\n",
        "---"
      ],
      "id": "f8bd57e1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Gathering\n",
        "\n",
        "I used twitter API to scratch mutiple keywords:**\"energy\" \"energy economy\" \"consumption\" \"data\" \"USA\"** to gather comments from twiter about their opinion about the energy consumption and energy economy. I use these data to analyze the positive and negative opinion. I can detect the internet users' attitudes to energy influence on economy. I can also use this datasets to define the relationship between economy and data. The keywords may not clear enough for me to analysis but I will adjust it later for future research. I used for loop to search over 600 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. I plan to detect the frequency of words to gain a plot. More than this, I plan to use Naive Bytes to give each tweet a positive or negative attitude.\n",
        "\n",
        "## Twitter API in Python\n",
        "The python package we will use in gathering data."
      ],
      "id": "fd33cf53"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import json \n",
        "import tweepy\n",
        "import requests\n",
        "from pandas import json_normalize \n",
        "import pandas as pd"
      ],
      "id": "709497b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the keys from Twitter and defined key words to scratch data into json file"
      ],
      "id": "a361b28e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# READ FILE\n",
        "f = open(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/codes/01-data-gathering/api-keys.json\")\n",
        "input=json.load(f); #print(input)\n",
        "\n",
        "# LOAD KEYS INTO API\n",
        "consumer_key=input[\"consumer_key\"]    \n",
        "consumer_secret=input[\"consumer_secret\"]    \n",
        "access_token=input[\"access_token\"]    \n",
        "access_token_secret=input[\"access_token_secret\"]    \n",
        "bearer_token=input[\"bearer_token\"]\n",
        "\n",
        "# Set up Connection\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Add the search_twitter function here.\n",
        "def search_twitter(query, max_results,tweet_fields, bearer_token = bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&{}\".format(query, max_results,tweet_fields)\n",
        "    print(\"--------------\",url,\"--------------\")\n",
        "    response = requests.request(\"GET\", url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "tweet_fields = \"tweet.fields=text,author_id,created_at,lang\"\n",
        "\n",
        "data = \"/Users/apple/Desktop/anly-501-project-HuitingSong-1/data/raw data/\"\n",
        "search_tweets = [\"energy\",\"energy economy\",\"consumption\",\"data\",\"USA\"]\n",
        "for idx,val in enumerate(search_tweets):\n",
        "    tweets_jsondump = []\n",
        "    json_response1 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response2 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response3 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response4 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response5 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response6 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    for i in json_response1['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response2['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response3['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response4['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response5['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response6['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    with open(data+str(val)+'.json','w') as json_file:\n",
        "        json.dump(tweets_jsondump,json_file)\n",
        "        json_file.close()"
      ],
      "id": "f0b03177",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalize the json file and form a data frame "
      ],
      "id": "02e743e4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "twitterdf1 = json_normalize(json_response1,\"data\")\n",
        "twitterdf2 = json_normalize(json_response2,\"data\")\n",
        "twitterdf3 = json_normalize(json_response3,\"data\")\n",
        "twitterdf4 = json_normalize(json_response4,\"data\")\n",
        "twitterdf5 = json_normalize(json_response5,\"data\")\n",
        "twitterdf6 = json_normalize(json_response6,\"data\")\n",
        "twitterdf = [twitterdf1,twitterdf2,twitterdf3,twitterdf4,twitterdf5,twitterdf6]\n",
        "twitterdf = pd.concat(twitterdf)\n",
        "twitterdf"
      ],
      "id": "f2c6f1f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save the data frame as csv file for futhur using"
      ],
      "id": "9fcb0131"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "twitterdf.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/data/raw data/twitterpython.csv\")"
      ],
      "id": "1d9a64a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Twitter API in R\n",
        "\n",
        "\n",
        "```{r message=FALSE, warning=FALSE, include=FALSE}\n",
        "library(selectr)\n",
        "library(rvest)\n",
        "library(xml2)\n",
        "library(rtweet) # for scraping tweets\n",
        "library(wordcloud2) # for generating really cool looking wordclouds\n",
        "library(tm) # for text minning\n",
        "library(dplyr) # loads of fun stuff including piping\n",
        "library(ROAuth)\n",
        "library(jsonlite)\n",
        "library(httpuv)\n",
        "library(twitteR)\n",
        "```\n",
        "\n",
        "```{r}\n",
        "consumer_key = \"zV2ER7zM1HzFUshDEHpJ8UuVS\"\n",
        "consumer_secret = \"5uHStDPwbFxEfLZojqPP9eDcWWXgfNCwdVPe8VG0xFvPldjhHJ\"\n",
        "access_token = \"1558952162692108289-bQDaHCqBuJZnXXqeQWnz7Pk1bhdiWm\"\n",
        "access_token_secret = \"kSJLq0tng5MfaFHoxbc7GpQ6D4SaplXzthDrW2SzhTSpD\"\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAEGthAEAAAAArntLVGGx5Q8Irzy%2BRTc2HdgQ7Vw%3DOR64kNsk2o0eqNl9KKZQKs7i2yzUoHEObJHwsQWiwesFtlQKdp\"\n",
        "\n",
        "requestURL='https://api.twitter.com/oauth/request_token'\n",
        "accessURL='https://api.twitter.com/oauth/access_token'\n",
        "authURL='https://api.twitter.com/oauth/authorize'\n",
        "\n",
        "s_key  = 'natural gas OR gasoline OR crude oil'\n",
        "n_tweets = 250 \n",
        "\n",
        "twitteR:::setup_twitter_oauth(consumer_key, consumer_secret,access_token,access_token_secret)\n",
        "Search1<-twitteR::searchTwitter(s_key,n=n_tweets, since=\"2022-05-01\",lang=\"en\")\n",
        "```\n",
        "\n",
        "```{r}\n",
        "TweetsDF<- twListToDF(Search1)\n",
        "TweetsDF\n",
        "write.csv(TweetsDF,\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/data/raw data/twitterR.csv\")\n",
        "```"
      ],
      "id": "74c9b3fa"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}