{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Clustering\"\n",
        "pdf-engine: lualatex\n",
        "format:\n",
        "  html:\n",
        "        code-fold: true\n",
        "        self-contained: true\n",
        "execute:\n",
        "    warning: false\n",
        "---"
      ],
      "id": "ed0f13ef"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introducation to Clustering\n",
        "\n",
        "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In simple words, the aim is to segregate groups with similar traits and assign them into clusters.\n",
        "\n",
        "Let’s understand this with an example. Suppose, you are the head of a rental store and wish to understand preferences of your costumers to scale up your business. Is it possible for you to look at details of each costumer and devise a unique business strategy for each one of them? Definitely not. But, what you can do is to cluster all of your costumers into say 10 groups based on their purchasing habits and use a separate strategy for costumers in each of these 10 groups. And this is what we call clustering.\n",
        "\n",
        "refernce: https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/ \n",
        "\n",
        "## Dataset Introduction\n",
        "\n",
        "#### New Dataset of Residential power usage 3 years data\n",
        "\n",
        "There are two datasets in total. The power usage data, extracted from “TRIEAGLE ENERGY LP, The Woodlands, Texas 77393”. and the historical weather data for Houston, Texas extracted from “www.wunderground.com”\n",
        "\n",
        "For the direction I want to study, I decided to use this ataset is because i want to know how residence use energy in time series. clustering can help me to cluster factors together in order to get more persuasive and accurate result to determine what energy consumption range resident usually is.\n",
        "\n",
        "Since there are too many factor or value influence the energy consumption, I categorize the value into three different groups in variable `usage`:\n",
        " - high (2): value in kWh >= 1.5 \n",
        " - low (0): value in kWh <= 1.5\n",
        "\n",
        "also, for better utilize in the future, i convert the `notes` into factors 0-4:\n",
        "  - weekday : 0\n",
        "  - weekend : 1\n",
        "  - vacation : 2\n",
        "  - COVID-lockdown : 3\n",
        "\n",
        "In these datasets, we will use three kinds of methods of clustering and compare their accuracy and feasibility to determine which method is the most suitable to do clustering and reach conclusions.\n",
        "\n",
        "The three methods of clustering: K means / DBSAN / Hierachical clustering\n",
        "\n",
        "## Process to clustering\n",
        "\n",
        "### K-Means Clustering\n",
        "\n",
        "K-means clustering (k-means) is a clustering algorithm based on the division of the sample set. K-means clustering divides the sample set into k subsets, which constitute k classes, and divides n samples into k classes, each sample has the smallest distance to the center of the class it belongs to, and each sample belongs to only one class, which is k-means clustering, \n",
        "\n",
        "This algorithm works in these 5 steps :\n",
        "    1. Specify the desired number of clusters K\n",
        "    2. Randomly assign each data point to a cluster\n",
        "    3. Compute cluster centroids\n",
        "    4. Re-assign each point to the closest cluster centroid\n",
        "    5. Re-compute cluster centroids\n"
      ],
      "id": "55d56e82"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from statistics import mode\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "df = pd.read_csv(\"/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/merged_w_u.csv\",index_col=[0])\n",
        "df.head()"
      ],
      "id": "294d13b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "clean and EDA\n",
        "\n",
        "no NA missing"
      ],
      "id": "2f6ce348"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.isna().sum()"
      ],
      "id": "41033250",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "drop columns"
      ],
      "id": "a1284555"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = df.drop([\"Day\",\"year\",\"time\"],axis=1)"
      ],
      "id": "e8fdf7b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "correlation matrix\n",
        "\n",
        "the graph shows that the usage and humidity are somehow positively correlated. and the pressure and temperature are negatively correlated.\n"
      ],
      "id": "c9de2701"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corr = df.corr();  \n",
        "print(corr.shape)\n",
        "sns.set_theme(style=\"white\")\n",
        "f, ax = plt.subplots(figsize=(20, 20)) \n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)   \n",
        "sns.heatmap(corr,  cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "plt.show()"
      ],
      "id": "59b0e079",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "split the dataset "
      ],
      "id": "bb5be98c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = df[['Value (kWh)','Temp_avg']]\n",
        "scalar = StandardScaler()\n",
        "scalar.fit(X)\n",
        "X = scalar.transform(X)"
      ],
      "id": "be032e03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fit ino the KMeans model and generate evaluation plot"
      ],
      "id": "ab156069"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "distortions = []\n",
        "inertias = []\n",
        "k = 10\n",
        "\n",
        "for k in range(1,k):\n",
        "    kmeanModel = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
        "    kmeanModel.fit(X)\n",
        "\n",
        "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'),axis=1))/X.shape[0])\n",
        "    inertias.append(kmeanModel.inertia_)\n",
        "    evaluation = pd.DataFrame.from_records({'Cluster':np.arange(1,k+1), 'Distortion':distortions, \"Inertia\":inertias})\n",
        "\n",
        "evaluation"
      ],
      "id": "a1d0587b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "evaluation.plot.line(x = 'Cluster', subplots=True)\n",
        "\n",
        "print('Elbow Method to determine the number of clusters to be formed:')\n",
        "Elbow_M = KElbowVisualizer(KMeans(), k=10)\n",
        "Elbow_M.fit(X)\n",
        "Elbow_M.show()\n",
        "\n",
        "bestK = KMeans(n_clusters=4, init='k-means++',random_state=42)\n",
        "labels4 = bestK.fit_predict(X)\n",
        "df['nlabels'] = labels4\n",
        "sns.scatterplot(x='usage',hue='nlabels',data=df)\n",
        "plt.show()"
      ],
      "id": "b0275269",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DBSCAN\n",
        "\n",
        "The idea is that assuming the categories can be determined by the closeness of the sample distribution, and that samples of the same category, which are closely related to each other, must exist in the same category not far from any sample of that category.\n",
        "\n",
        "The advantages of DBSCAN are that it does not require a predetermined number of clusters as KMeans does, and is insensitive to outliers. Also he can separate high density data into small clusters and can also cluster non-linear relationships (clustering to arbitrary shapes). However, it also has disadvantages because it is difficult to identify clusters in data of different densities, difficult to cluster high-dimensional data, and very sensitive to parameters of very small points.\n",
        "\n",
        "perform DBSCAN clustering. use the eps and min_samples parameters to find the optimal number of clusters. plot the number of clusters vs the silhouette score. Suggest the optimal number of clusters based on the plot.\n"
      ],
      "id": "b626488d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "eps_range = np.arange(1,11)\n",
        "for i in eps_range:\n",
        "    db = DBSCAN(eps=i, min_samples=5).fit(X)\n",
        "    labels = db.labels_\n",
        "    if len(np.unique(labels)) == 1:\n",
        "        continue\n",
        "    silhouette_avg = silhouette_score(X, labels)\n",
        "    print(\n",
        "        \"For eps = {:0.2f}\".format(i),\n",
        "        \"The average silhouette_score is {:0.4f}.\".format(silhouette_avg)\n",
        "    )"
      ],
      "id": "fd8ca078",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "for the result \n",
        "\n",
        "### Hierarchical Clustering\n",
        "\n",
        "As the name implies, the clustering is done layer by layer, either by splitting the large categories (cluster) from top to bottom or by aggregating the small categories from bottom to top. Hierarchical clustering methods decompose a given data set hierarchically until certain conditions are met. After distance values have been obtained, elements can be linked to each other. A structure can be constructed by separation and fusion. The method traditionally represented is a Tree data structure.\n",
        "\n",
        "Hierarchical clustering algorithms are either bottom-up aggregation type, which starts from leaf nodes and eventually converges to the root node, or top-down splitting type, which starts from the root node and recursively splits downward.\n"
      ],
      "id": "e66f4193"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "envname",
      "language": "python",
      "display_name": "envname"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}