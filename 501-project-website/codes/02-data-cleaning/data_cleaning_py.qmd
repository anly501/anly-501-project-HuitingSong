---
title: "Data Cleaning"
pdf-engine: lualatex
format:
  html:
        code-fold: False
        self-contained: true
execute:
    warning: false
---

**Data cleaning** is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled. If data is incorrect, outcomes and algorithms are unreliable, even though they may look correct. There is no one absolute way to prescribe the exact steps in the data cleaning process because the processes will vary from dataset to dataset. But it is crucial to establish a template for your data cleaning process so you know you are doing it the right way every time. In this section, I will clean my text data and modified data. The steps for cleaning these two kind of data is different, so I will specify the cleaning steps with presenting the code. 

## Python on Text Data 

#### Dataset "twitterpython.csv" which is grabbed through python

People's perception of energy consumption is very important. It not only reflects the public's perception, but also indirectly reflects people's concern for environmental issues and their willingness to change the status quo. In the data gathering section, I have gather the text data by setting "enviornment" "clean energy" "conventional energy" "energy consumption" as the keywords for the twitter api to get the text data. Now I will do the data processing for the text dataset through python. 

 - Step 1: import the packages we may use during data processing
```{python}
import pandas as pd
import sklearn
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np
from sklearn import svm
import nltk
import warnings
warnings.filterwarnings('ignore')
from nltk.sentiment import SentimentIntensityAnalyzer
from cleantext import clean
```

 - Step 2: Load the Dataset we grab by using python
```{python}
twitter = pd.read_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/raw data/twitterpython.csv")
twitter.head()
```

 - Step 3: Find the dataset with NA missing value
 
 Through checking, there is no missing value in the text dataset.

```{python}
twitter.isnull().sum()
```

 - Step 4: Pre-processing the text info in the dataset
 
 Since the raw data contains lots of puntuations, urls, commas, numbers, highercase, and some other things which will influence our analysis and tokenization. Therefore, we need to remove the unnecessary things. Since this semester, I take Natural Language Processing course, and learn how to use **pipline** to process the text information. So, in this part, I will the method I learn from NLP. 
 
 Through pipline, the things which will influence our analysis will be cleaned and replaced. And then, we will need to transform the sentence from text data to tokens which is a list of words through tokenizer.

```{python}
import re
import spacy
from spacy.language import Language

pipeline = spacy.load('en_core_web_sm')

# http://emailregex.com/
email_re = r"""(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])"""

# http://urlregex.com/
url_re = r"""http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"""

emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U0001F1F2-\U0001F1F4"  # Macau flag
        u"\U0001F1E6-\U0001F1FF"  # flags
        u"\U0001F600-\U0001F64F"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U0001F1F2"
        u"\U0001F1F4"
        u"\U0001F620"
        u"\u200d"
        u"\u2640-\u2642"
        "]+", flags=re.UNICODE)

# replace = [ (pattern-to-replace, replacement),  ...]
replace = [
    (url_re, ""),                # Matches most URLs
    (email_re, ""),            # Matches emails
    (emoji_pattern,""),             # remove emoji
    (r"(?<=\d),(?=\d)", ""),        # Remove commas in numbers
    (r"\d+", ""),              # Map digits to special token <numbr>
    (r"[\t\n\r\*\.\@\,\-\/]", " "), # Punctuation and other junk
    (r"\s+", " ")                   # Stips extra whitespace
]

twitter_sentences = []
for i, d in enumerate(twitter['text']):
    for repl in replace:
        d = re.sub(repl[0], repl[1], d)
    twitter_sentences.append(d)

@Language.component("lab04Preprocessor")
def ng20_preprocess(doc):
    tokens = [token for token in doc 
              if not any((token.is_stop, token.is_punct))]
    tokens = [token.lemma_.lower().strip() for token in tokens]
    tokens = [token for token in tokens if token]
    return " ".join(tokens)

pipeline.add_pipe("lab04Preprocessor")
```

 - Step 5: Pass data through spacy pipeline

 Since I found that there still have some emoji that have not been cleaned after viewing the trying result, I add the a line from packages clean_text. This packages can efficiently remove all the emoji by choosing the `no_emoji = True`. Now, the text are all be cleaned. 
```{python}
docs = []
for sent in twitter_sentences:
    s = clean(sent,no_emoji=True,no_currency_symbols=True,no_punct=True) # remove all the emoji
    docs.append(pipeline(s))

result = pd.DataFrame(docs)
result.head()
```

 
 - Step 6: Backup the processed data
```{python}
result.to_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/piplineresult.csv")
```

 - Step 7: Using the CountVectorizer and count the words freqency

 Countvectorizer can convert a collection of text documents to a matrix of token counts.
 The key in bag of word vocabulary is the token which is also the word in text. The value is the count of the key which is calculated by fitting the cleaned text in to countvectorizer. After sorting the counts, we can see the frequency of each words or tokens. 

```{python}
vectorizer = CountVectorizer()
bow = vectorizer.fit(docs)
features = bow.vocabulary_.keys()
counts = bow.vocabulary_.values()
bow=pd.DataFrame({'words':features,'counts':counts})
bow = bow.sort_values(by=['counts'],ascending=False)
bow
```

 - Step 8: Backup the processed data 
```{python}
bow.to_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/countword.csv")
```

 - Step 9: Opinion Mining or Sentiment Analysis
 
 Opinion mining is an approach to natural language processing (NLP) that identifies the emotional tone behind a body of text. This is a popular way for organizations to determine and categorize opinions about a product, service, or idea. In addition to identifying sentiment, opinion mining can extract the polarity (or the amount of positivity and negativity), subject and opinion holder within the text. Furthermore, sentiment analysis can be applied to varying scopes such as document, paragraph, sentence and sub-sentence levels.

 There are four most popular NLP Sentiment analysis packages:
    - Text Blob
    - VADER
    - Flair
    - Custom Model: such as Naive Bayes, SVM, etc. 
 
 I will choose VADER to do the sentiment analysis on my twitter text data. VADER uses a list of lexical features (e.g. word) which are labeled as positive or negative according to their semantic orientation to calculate the text sentiment. It will return the probability of a given  input sentence to be `positive`,`negative`,`neutral`. 

 **Note:** This sentiment analysis is to only for simple analysis. The accuracy for detecting the sentiment of each input sentence will not be investigated. VADER only implement the sentiment rule to the word from their orientation, so this means that this method does not have the ability to analysis the opinion of input as sentence unit. Since the text data I grab from Twitter is without label of users' attitude. Thus, in this section, i just need the sentiment score the algorithm generated for future research. 

 According to my project direction and ten question, sentiment analysis can help me to answer the question of how twitter users' think about the energy consumption and how likely they are going to do action for protecting enviornment. Thus, understanding twitter users' opinion is important step in doing my research. 

 - use the cleaned text to do the analysis. otherwise, the sentiment score will be influenced. 
```{python}
twtpy = pd.read_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/piplineresult.csv",index_col=[0])
twtpy.head()
```
 - change the column name and get the sentiment score
```{python}
twtpy.rename(columns={"0":'clean_text'},inplace=True)
twtpy.head()
```
 - defined the function
```{python}
 # define the function
def getSentiments(df):
    sid = SentimentIntensityAnalyzer()
    tweet_str = ""
    tweetscore = []
    for tweet in df['clean_text']:
        tweet_str = tweet_str + " " + tweet
        score = sid.polarity_scores(tweet_str)
        tweetscore.append(score)
    return tweetscore

# call the function above to see the result score and form a data frame to record
sentiment = getSentiments(twtpy)
texts = pd.DataFrame(twitter['text'])
clean_text = pd.DataFrame(twtpy['clean_text'])
stmscore = pd.DataFrame.from_dict(sentiment)
stmscore.head()
```
 - concat the original text, clean_text, and the sentiment score 
```{python}
# relate the text and score for better view
txtscore = pd.concat([texts,clean_text,stmscore],axis=1)
twtpy.rename(columns={},inplace=True)
txtscore.head()
```
 - save the dataframe for future use
```{python}
# export the csv for future analysis
txtscore.to_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/pystmscore.csv")
```


#### Dataset "twitterR.csv" which is grabbed through python

For Twitter API in R, I used twitter to scratch keywords like **"oil","gas","solar power","wind power"** to scratch usersâ€™ attitudes about these four kinds of energy. I want to search different types of energy in order to analysis people attitude toward different enegy, and what they think about two different kind of energy (renewable energy and convential energy).

Since the dataset has gone through word counts in `record data in R` link, The purpose for the dataset here is to do the sentiment analysis, which is opinion mining. An important analysis for my project in understanding twitter users' idea about different type of energy.

- load the dataset
```{python}
twtr = pd.read_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/raw data/twitterR.csv")
twtr.head()
```

```{python}
import re
import spacy
from spacy.language import Language

pipeline = spacy.load('en_core_web_sm')

# http://emailregex.com/
email_re = r"""(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\])"""

# http://urlregex.com/
url_re = r"""http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"""

emoji_pattern = re.compile("["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
        u"\U0001F1F2-\U0001F1F4"  # Macau flag
        u"\U0001F1E6-\U0001F1FF"  # flags
        u"\U0001F600-\U0001F64F"
        u"\U00002702-\U000027B0"
        u"\U000024C2-\U0001F251"
        u"\U0001f926-\U0001f937"
        u"\U0001F1F2"
        u"\U0001F1F4"
        u"\U0001F620"
        u"\u200d"
        u"\u2640-\u2642"
        "]+", flags=re.UNICODE)

# replace = [ (pattern-to-replace, replacement),  ...]
replace = [
    (url_re, ""),                # Matches most URLs
    (email_re, ""),            # Matches emails
    (emoji_pattern,""),             # remove emoji
    (r"(?<=\d),(?=\d)", ""),        # Remove commas in numbers
    (r"\d+", ""),              # Map digits to special token <numbr>
    (r"[\t\n\r\*\.\@\,\-\/]", " "), # Punctuation and other junk
    (r"\s+", " ")                   # Stips extra whitespace
]

twitter_sentences = []
for i, d in enumerate(twtr['text']):
    for repl in replace:
        d = re.sub(repl[0], repl[1], d)
    twitter_sentences.append(d)

@Language.component("lab04Preprocessor")
def ng20_preprocess(doc):
    tokens = [token for token in doc 
              if not any((token.is_stop, token.is_punct))]
    tokens = [token.lemma_.lower().strip() for token in tokens]
    tokens = [token for token in tokens if token]
    return " ".join(tokens)

pipeline.add_pipe("lab04Preprocessor")

docs = []
for sent in twitter_sentences:
    s = clean(sent,no_emoji=True,no_currency_symbols=True,no_punct=True) # remove all the emoji
    docs.append(pipeline(s))

result = pd.DataFrame(docs)
result.head()
```
```{python}
result.to_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/piplineresult_r.csv")
```

```{python}
twtrr = pd.read_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/piplineresult_r.csv",index_col=[0])
twtrr.rename(columns={"0":'clean_text'},inplace=True)
twtrr.head()
```

```{python}
# define the function
def getSentiments(df):
    sid = SentimentIntensityAnalyzer()
    tweet_str = ""
    tweetscore = []
    for tweet in df['clean_text']:
        tweet_str = tweet_str + " " + tweet
        score = sid.polarity_scores(tweet_str)
        tweetscore.append(score)
    return tweetscore

# call the function above to see the result score and form a data frame to record
sentiment = getSentiments(twtrr)
texts = pd.DataFrame(twtr['text'])
clean_text = pd.DataFrame(twtrr['clean_text'])
stmscore = pd.DataFrame.from_dict(sentiment)
stmscore.head()

# relate the text and score for better view
txtscore = pd.concat([texts,clean_text,stmscore],axis=1)
print(txtscore.head())

# export the csv for future analysis
txtscore.to_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/rstmscore.csv")
```