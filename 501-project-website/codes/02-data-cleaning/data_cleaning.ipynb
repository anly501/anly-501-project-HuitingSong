{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data Gathering\"\n",
        "pdf-engine: lualatex\n",
        "format:\n",
        "  html:\n",
        "        code-fold: true\n",
        "        self-contained: true\n",
        "execute:\n",
        "    warning: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{r setup, include=FALSE}\n",
        "knitr::opts_chunk$set(echo = TRUE)\n",
        "reticulate::use_python(\"/usr/local/bin/python3\")\n",
        "```\n",
        "\n",
        "**Data cleaning** is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled. If data is incorrect, outcomes and algorithms are unreliable, even though they may look correct. There is no one absolute way to prescribe the exact steps in the data cleaning process because the processes will vary from dataset to dataset. But it is crucial to establish a template for your data cleaning process so you know you are doing it the right way every time. In this section, I will clean my text data and modified data. The steps for cleaning these two kind of data is different, so I will specify the cleaning steps with presenting the code. \n",
        "\n",
        "## Part 1 - R on downloaded dataset\n",
        "\n",
        "Load the possible packages\n",
        "```{r}\n",
        "library(tidyverse)\n",
        "library(dplyr)\n",
        "library(reshape2)\n",
        "library(tidyr)\n",
        "library(stringr)\n",
        "```\n",
        "\n",
        "#### Energy Consumption by sector dataset\n",
        "\n",
        "This dataset contains the info of value of monthly energy consumption by different sectors. I will use this official data set from U.S. EIA to see how exact the monthly usage of different energy in different sectors. Since the dataset is too dirty and I cannot tell what kind of variables it has now, let's do the cleaning and tidying step by step. Finally, I will introduce the variables I have in the dataset.\n",
        "\n",
        " - Step 1: load the dataset\n",
        "```{r}\n",
        "sector <- read.csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/downloaded data/consumptionbysector.csv\")\n",
        "head(sector,5)\n",
        "```\n",
        "\n",
        " - Step 2: check the NA missing value. If have, remove it. \n",
        "\n",
        " Through checking, No NA missing value in the dataset\n",
        "```{r}\n",
        "sum(is.na(sector))\n",
        "```\n",
        "\n",
        " - Step 3: Remove the irrelevent columns\n",
        " ```{r}\n",
        "sector.clean <- sector[ , ! names(sector) %in% c(\"MSN\",\"Column_Order\")]\n",
        "head(sector.clean)\n",
        " ```\n",
        " \n",
        " - Step 4: drop the value in column\n",
        "\n",
        " Since the column \"Description\" in this dataset contain the info of energy type and sector type AND there are more than 10000 dataset, I want to remove the irrelevent row with some value in this column.\n",
        "\n",
        " I check the levels of the column and there are totally 15 levels. Among the levels, there are lots of contains that I do not need. \n",
        " ```{r}\n",
        " unique(sector$Description)\n",
        " ```\n",
        "\n",
        " Remove the value i do not need in \"Description\"\n",
        " ```{r}\n",
        "sector.clean <- sector.clean %>%\n",
        "         filter( Description != \"Electricity Sales to Ultimate Customers in the Residential Sector\")%>%\n",
        "         filter( Description != \"Electricity Sales to Ultimate Customers in the Commercial Sector\")%>%\n",
        "         filter( Description != \"Electricity Sales to Ultimate Customers in the Industrial Sector\")\n",
        " ```\n",
        "\n",
        " - Step 5: split the column \"Description\" into two columns \"sector\" and \"energy type\"\n",
        " ```{r}\n",
        " sector.clean[c('energy_type', 'sector')] <- str_split_fixed(sector.clean$Description, 'by', 2)\n",
        " ```\n",
        "\n",
        " - Step 6: change the variable levels name and drop the column \"Description\"\n",
        " ```{r}\n",
        " sector.clean$energy_type[sector.clean$energy_type==\"Primary Energy Consumed\"]<-\"Primary Energy\"\n",
        " sector.clean$energy_type[sector.clean$energy_type==\"End-Use Energy Consumed\"]<-\"End-Use Energy\"\n",
        " sector.clean$energy_type[sector.clean$energy_type==\"Electrical System Energy Losses\"]<-\"Electrical Energy\"\n",
        " sector.clean$sector[sector.clean$sector==\"the Residential Sector\"]<-\"Residential\"\n",
        " sector.clean$sector[sector.clean$sector==\"the Commercial Sector\"]<-\"Commercial\"\n",
        " sector.clean$sector[sector.clean$sector==\"the Industrial Sector\"]<-\"Industrial\"\n",
        "\n",
        " sector.clean <- sector.clean[ , ! names(sector.clean) %in% c(\"Description\")]\n",
        " head(sector.clean)\n",
        " ```\n",
        " - Step 7: save to csv file\n",
        " ```{r}\n",
        " write.csv(sector.clean,\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/cleansector.csv\")\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>YYYYMM</th>\n",
              "      <th>Value</th>\n",
              "      <th>Unit</th>\n",
              "      <th>energy_type</th>\n",
              "      <th>sector</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>197301</td>\n",
              "      <td>1339.270</td>\n",
              "      <td>Trillion Btu</td>\n",
              "      <td>Primary Energy Consumed</td>\n",
              "      <td>the Residential Sector</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>197302</td>\n",
              "      <td>1174.899</td>\n",
              "      <td>Trillion Btu</td>\n",
              "      <td>Primary Energy Consumed</td>\n",
              "      <td>the Residential Sector</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>197303</td>\n",
              "      <td>983.374</td>\n",
              "      <td>Trillion Btu</td>\n",
              "      <td>Primary Energy Consumed</td>\n",
              "      <td>the Residential Sector</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>197304</td>\n",
              "      <td>715.391</td>\n",
              "      <td>Trillion Btu</td>\n",
              "      <td>Primary Energy Consumed</td>\n",
              "      <td>the Residential Sector</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>197305</td>\n",
              "      <td>535.914</td>\n",
              "      <td>Trillion Btu</td>\n",
              "      <td>Primary Energy Consumed</td>\n",
              "      <td>the Residential Sector</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   YYYYMM     Value          Unit               energy_type  \\\n",
              "1  197301  1339.270  Trillion Btu  Primary Energy Consumed    \n",
              "2  197302  1174.899  Trillion Btu  Primary Energy Consumed    \n",
              "3  197303   983.374  Trillion Btu  Primary Energy Consumed    \n",
              "4  197304   715.391  Trillion Btu  Primary Energy Consumed    \n",
              "5  197305   535.914  Trillion Btu  Primary Energy Consumed    \n",
              "\n",
              "                    sector  \n",
              "1   the Residential Sector  \n",
              "2   the Residential Sector  \n",
              "3   the Residential Sector  \n",
              "4   the Residential Sector  \n",
              "5   the Residential Sector  "
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "cleansector = pd.read_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/cleansector.csv\",index_col=[0])\n",
        "cleansector.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2 - Python on Text Data \n",
        "\n",
        "#### Dataset \"twitterpython.csv\" which is grabbed through python\n",
        "\n",
        "People's perception of energy consumption is very important. It not only reflects the public's perception, but also indirectly reflects people's concern for environmental issues and their willingness to change the status quo. In the data gathering section, I have gather the text data by setting “energy consumption” “global warming” “clean energy” “conventional energy” “environmental protection” as the keywords for the twitter api to get the text data. Now I will do the data processing for the text dataset through python. \n",
        "\n",
        " - Step 1: import the packages we may use during data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 2: Load the Dataset we grab by using python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author_id</th>\n",
              "      <th>text</th>\n",
              "      <th>created_at</th>\n",
              "      <th>id</th>\n",
              "      <th>edit_history_tweet_ids</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1549857918442524672</td>\n",
              "      <td>@bjgalloway1717 @abbieasr Global warming? Thou...</td>\n",
              "      <td>2022-12-07T10:25:57.000Z</td>\n",
              "      <td>1600436443490328576</td>\n",
              "      <td>['1600436443490328576']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1075616751663104000</td>\n",
              "      <td>@shirtsthtgohard i'm pretty excited for global...</td>\n",
              "      <td>2022-12-07T10:25:51.000Z</td>\n",
              "      <td>1600436419838717952</td>\n",
              "      <td>['1600436419838717952']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1515416161281785859</td>\n",
              "      <td>@JustinTrudeau What kind of idiot would burden...</td>\n",
              "      <td>2022-12-07T10:25:42.000Z</td>\n",
              "      <td>1600436381263663104</td>\n",
              "      <td>['1600436381263663104']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>105241093</td>\n",
              "      <td>What beautiful but 'scary' visualisation of ea...</td>\n",
              "      <td>2022-12-07T10:25:31.000Z</td>\n",
              "      <td>1600436336145465344</td>\n",
              "      <td>['1600436336145465344']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18852344</td>\n",
              "      <td>@DawnNeesom Don't forget if it Snows in Decemb...</td>\n",
              "      <td>2022-12-07T10:25:24.000Z</td>\n",
              "      <td>1600436304793153536</td>\n",
              "      <td>['1600436304793153536']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             author_id                                               text  \\\n",
              "0  1549857918442524672  @bjgalloway1717 @abbieasr Global warming? Thou...   \n",
              "1  1075616751663104000  @shirtsthtgohard i'm pretty excited for global...   \n",
              "2  1515416161281785859  @JustinTrudeau What kind of idiot would burden...   \n",
              "3            105241093  What beautiful but 'scary' visualisation of ea...   \n",
              "4             18852344  @DawnNeesom Don't forget if it Snows in Decemb...   \n",
              "\n",
              "                 created_at                   id   edit_history_tweet_ids lang  \n",
              "0  2022-12-07T10:25:57.000Z  1600436443490328576  ['1600436443490328576']   en  \n",
              "1  2022-12-07T10:25:51.000Z  1600436419838717952  ['1600436419838717952']   en  \n",
              "2  2022-12-07T10:25:42.000Z  1600436381263663104  ['1600436381263663104']   en  \n",
              "3  2022-12-07T10:25:31.000Z  1600436336145465344  ['1600436336145465344']   en  \n",
              "4  2022-12-07T10:25:24.000Z  1600436304793153536  ['1600436304793153536']   en  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "twitter = pd.read_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/raw data/twitterpython.csv\",index_col=[0])\n",
        "twitter.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 3: Find the dataset with NA missing value\n",
        " \n",
        " Through checking, there is no missing value in the text dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "author_id                 0\n",
              "text                      0\n",
              "created_at                0\n",
              "id                        0\n",
              "edit_history_tweet_ids    0\n",
              "lang                      0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "twitter.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 4: Remove data with language other than English\n",
        "\n",
        " Since my Twitter API collect the text with different languages, I need to drop the text information that cannot be understand by me and you. I only keep the text with English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author_id</th>\n",
              "      <th>text</th>\n",
              "      <th>created_at</th>\n",
              "      <th>id</th>\n",
              "      <th>edit_history_tweet_ids</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1549857918442524672</td>\n",
              "      <td>@bjgalloway1717 @abbieasr Global warming? Thou...</td>\n",
              "      <td>2022-12-07T10:25:57.000Z</td>\n",
              "      <td>1600436443490328576</td>\n",
              "      <td>['1600436443490328576']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1075616751663104000</td>\n",
              "      <td>@shirtsthtgohard i'm pretty excited for global...</td>\n",
              "      <td>2022-12-07T10:25:51.000Z</td>\n",
              "      <td>1600436419838717952</td>\n",
              "      <td>['1600436419838717952']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1515416161281785859</td>\n",
              "      <td>@JustinTrudeau What kind of idiot would burden...</td>\n",
              "      <td>2022-12-07T10:25:42.000Z</td>\n",
              "      <td>1600436381263663104</td>\n",
              "      <td>['1600436381263663104']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>105241093</td>\n",
              "      <td>What beautiful but 'scary' visualisation of ea...</td>\n",
              "      <td>2022-12-07T10:25:31.000Z</td>\n",
              "      <td>1600436336145465344</td>\n",
              "      <td>['1600436336145465344']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>18852344</td>\n",
              "      <td>@DawnNeesom Don't forget if it Snows in Decemb...</td>\n",
              "      <td>2022-12-07T10:25:24.000Z</td>\n",
              "      <td>1600436304793153536</td>\n",
              "      <td>['1600436304793153536']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>152971284</td>\n",
              "      <td>You lot. You spend all your time thinking abou...</td>\n",
              "      <td>2022-12-07T10:02:28.000Z</td>\n",
              "      <td>1600430533154025472</td>\n",
              "      <td>['1600430533154025472']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>3402582670</td>\n",
              "      <td>RT @Dad_At_Law: First voicemail, next up, glob...</td>\n",
              "      <td>2022-12-07T10:02:27.000Z</td>\n",
              "      <td>1600430530016657408</td>\n",
              "      <td>['1600430530016657408']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>1181913812657565707</td>\n",
              "      <td>for the first time ever; i’m finally getting w...</td>\n",
              "      <td>2022-12-07T10:02:26.000Z</td>\n",
              "      <td>1600430526690562048</td>\n",
              "      <td>['1600430526690562048']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>947078072595034112</td>\n",
              "      <td>@Lindhacker @ZLabe @NOAA_ESRL Re: \"Thermal dyn...</td>\n",
              "      <td>2022-12-07T10:01:17.000Z</td>\n",
              "      <td>1600430235165810689</td>\n",
              "      <td>['1600430235165810689']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>1119378174540906497</td>\n",
              "      <td>waiting for global warming to just end it all</td>\n",
              "      <td>2022-12-07T10:01:03.000Z</td>\n",
              "      <td>1600430175497629696</td>\n",
              "      <td>['1600430175497629696']</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>552 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              author_id                                               text  \\\n",
              "0   1549857918442524672  @bjgalloway1717 @abbieasr Global warming? Thou...   \n",
              "1   1075616751663104000  @shirtsthtgohard i'm pretty excited for global...   \n",
              "2   1515416161281785859  @JustinTrudeau What kind of idiot would burden...   \n",
              "3             105241093  What beautiful but 'scary' visualisation of ea...   \n",
              "4              18852344  @DawnNeesom Don't forget if it Snows in Decemb...   \n",
              "..                  ...                                                ...   \n",
              "95            152971284  You lot. You spend all your time thinking abou...   \n",
              "96           3402582670  RT @Dad_At_Law: First voicemail, next up, glob...   \n",
              "97  1181913812657565707  for the first time ever; i’m finally getting w...   \n",
              "98   947078072595034112  @Lindhacker @ZLabe @NOAA_ESRL Re: \"Thermal dyn...   \n",
              "99  1119378174540906497      waiting for global warming to just end it all   \n",
              "\n",
              "                  created_at                   id   edit_history_tweet_ids  \\\n",
              "0   2022-12-07T10:25:57.000Z  1600436443490328576  ['1600436443490328576']   \n",
              "1   2022-12-07T10:25:51.000Z  1600436419838717952  ['1600436419838717952']   \n",
              "2   2022-12-07T10:25:42.000Z  1600436381263663104  ['1600436381263663104']   \n",
              "3   2022-12-07T10:25:31.000Z  1600436336145465344  ['1600436336145465344']   \n",
              "4   2022-12-07T10:25:24.000Z  1600436304793153536  ['1600436304793153536']   \n",
              "..                       ...                  ...                      ...   \n",
              "95  2022-12-07T10:02:28.000Z  1600430533154025472  ['1600430533154025472']   \n",
              "96  2022-12-07T10:02:27.000Z  1600430530016657408  ['1600430530016657408']   \n",
              "97  2022-12-07T10:02:26.000Z  1600430526690562048  ['1600430526690562048']   \n",
              "98  2022-12-07T10:01:17.000Z  1600430235165810689  ['1600430235165810689']   \n",
              "99  2022-12-07T10:01:03.000Z  1600430175497629696  ['1600430175497629696']   \n",
              "\n",
              "   lang  \n",
              "0    en  \n",
              "1    en  \n",
              "2    en  \n",
              "3    en  \n",
              "4    en  \n",
              "..  ...  \n",
              "95   en  \n",
              "96   en  \n",
              "97   en  \n",
              "98   en  \n",
              "99   en  \n",
              "\n",
              "[552 rows x 6 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "twitter = twitter[twitter.lang == \"en\"]\n",
        "twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 4: Pre-processing the text info in the dataset\n",
        " \n",
        " Since the raw data contains lots of puntuations, urls, commas, numbers, highercase, and some other things which will influence our analysis and tokenization. Therefore, we need to remove the unnecessary things. Since this semester, I take Natural Language Processing course, and learn how to use **pipline** to process the text information. So, in this part, I will the method I learn from NLP. \n",
        " \n",
        " Through pipline, the things which will influence our analysis will be cleaned and replaced. And then, we will need to transform the sentence from text data to tokens which is a list of words through tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function __main__.ng20_preprocess(doc)>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "pipeline = spacy.load('en_core_web_sm')\n",
        "\n",
        "# http://emailregex.com/\n",
        "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
        "\n",
        "# replace = [ (pattern-to-replace, replacement),  ...]\n",
        "replace = [\n",
        "    (r\"<a[^>]*>(.*?)</a>\", r\"\\1\"),  # Matches most URLs\n",
        "    (email_re, \"email\"),            # Matches emails\n",
        "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
        "    (r\"\\d+\", \"number\"),              # Map digits to special token <numbr>\n",
        "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n",
        "    (r\"\\s+\", \" \")                   # Stips extra whitespace\n",
        "]\n",
        "\n",
        "twitter_sentences = []\n",
        "for i, d in enumerate(twitter['text']):\n",
        "    for repl in replace:\n",
        "        d = re.sub(repl[0], repl[1], d)\n",
        "    twitter_sentences.append(d)\n",
        "\n",
        "@Language.component(\"lab04Preprocessor\")\n",
        "def ng20_preprocess(doc):\n",
        "    tokens = [token for token in doc \n",
        "              if not any((token.is_stop, token.is_punct))]\n",
        "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "pipeline.add_pipe(\"lab04Preprocessor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 5: Pass data through spacy pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'twitter_sentences' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/codes/02-data-cleaning/data_cleaning.ipynb Cell 15\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/codes/02-data-cleaning/data_cleaning.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m docs \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/codes/02-data-cleaning/data_cleaning.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m twitter_sentences:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/codes/02-data-cleaning/data_cleaning.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     docs\u001b[39m.\u001b[39mappend(pipeline(sent))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/codes/02-data-cleaning/data_cleaning.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(docs)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'twitter_sentences' is not defined"
          ]
        }
      ],
      "source": [
        "docs = []\n",
        "for sent in twitter_sentences:\n",
        "    docs.append(pipeline(sent))\n",
        "\n",
        "result = pd.DataFrame(docs)\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 6: Backup the processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/piplineresult.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 7: Using the CountVectorizer and count the words freqency\n",
        " \n",
        " Countvectorizer can convert a collection of text documents to a matrix of token counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit(docs)\n",
        "features = bow.vocabulary_.keys()\n",
        "counts = bow.vocabulary_.values()\n",
        "bow=pd.DataFrame({'words':features,'counts':counts})\n",
        "bow = bow.sort_values(by=['counts'],ascending=False)\n",
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 8: Backup the processed data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bow.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/countword.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " - Step 9: Opinion Mining\n",
        " \n",
        " Opinion mining is an approach to natural language processing (NLP) that identifies the emotional tone behind a body of text. This is a popular way for organizations to determine and categorize opinions about a product, service, or idea. In addition to identifying sentiment, opinion mining can extract the polarity (or the amount of positivity and negativity), subject and opinion holder within the text. Furthermore, sentiment analysis can be applied to varying scopes such as document, paragraph, sentence and sub-sentence levels.\n",
        "\n",
        " Since I would like to analysis the attitude people toward the energy consumption, opinion mining is an useful methods to conduct the result. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "md-indent": " "
      },
      "outputs": [],
      "source": [
        "# define the function\n",
        "defgetSentiments(df):\n",
        "   sid = SentimentIntensityAnalyzer()\n",
        "   tweet_str = \"\"\n",
        "   tweetscore = []\n",
        "   for tweet in df['text']:\n",
        "       tweet_str = tweet_str + \" \" + tweet\n",
        "       score = sid.polarity_scores(tweet_str)\n",
        "       tweetscore.append(score)\n",
        "   return tweetscore\n",
        "\n",
        "#call the function above to see the result score and form a data frame to record\n",
        "sentiment= getSentiments(twitter)\n",
        "texts= pd.DataFrame(twitter.text)\n",
        "stmscore= pd.DataFrame.from_dict(sentiment)\n",
        "stmscore.head()\n",
        "\n",
        "#relate the text and score for better view\n",
        "txtscore= pd.concat([texts,stmscore],axis=1)\n",
        "txtscore.head()\n",
        "\n",
        "#export the csv for future analysis\n",
        "txtscore.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaneddata/pystmscore.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Dataset \"twitterR.csv\" which is grabbed by R\n",
        "\n",
        "For Twitter API in R, I used twitter to scratch keywords like **\"oil\",\"gas\",\"solar power\",\"wind power\"** to scratch users’ attitudes about these four kinds of energy. I want to search different types of energy in order to analysis people attitude toward different enegy, and what they think about two different kind of energy (renewable energy and convential energy).\n",
        "\n",
        " - Step 1: import the packages we may use during data processing\n",
        " ```{r}\n",
        " library(selectr)\n",
        "library(rvest)\n",
        "library(xml2)\n",
        "library(wordcloud2) # for generating really cool looking wordclouds\n",
        "library(tm) # for text minning\n",
        "library(dplyr) \n",
        "library(ROAuth)\n",
        "library(jsonlite)\n",
        "library(httpuv)\n",
        "suppressWarnings(expr)\n",
        " ```\n",
        "\n",
        " - Step 2: Load the Dataset we grab by using R\n",
        " ```{r}\n",
        " twtr <- read.csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/raw data/twitterR.csv\")\n",
        " head(twtr,5)\n",
        " ```\n",
        "\n",
        " - Step 3: Find the dataset with NA missing value\n",
        " \n",
        " Since R collects lots of basic infor of users and tweet situation, we just need to check whether there are missing value in text variable. Through checking, there is no missing value in the text.\n",
        " ```{r}\n",
        " sum(is.na(twtr$text))\n",
        " ```\n",
        " \n",
        " - Step 4: Pre-processing the text info in the dataset\n",
        "```{r}\n",
        "twittertext = Corpus(VectorSource(twtr$text))\n",
        "toSpace = content_transformer(\n",
        "              function (x, pattern)\n",
        "              gsub(pattern, \" \", x))\n",
        "twittertext1 = tm_map(twittertext, toSpace, \"/\")\n",
        "twittertext1 = tm_map(twittertext, toSpace, \"@\")\n",
        "twittertext1 = tm_map(twittertext, toSpace, \"#\")\n",
        "twittertext1 = tm_map(twittertext1, content_transformer(tolower))\n",
        "twittertext1 = tm_map(twittertext1, removeNumbers)\n",
        "twittertext1 = tm_map(twittertext1, stripWhitespace)\n",
        "removeURL <- function(x) gsub(\"http[[:alnum:]]*\", \"\", x)\n",
        "text = tm_map(twittertext, removeURL)\n",
        "head(text)\n",
        "```\n",
        "\n",
        " - Step 5: count the words freqency\n",
        "```{r}\n",
        "term = TermDocumentMatrix(text)\n",
        "m = as.matrix(term)\n",
        "v = sort(rowSums(m), \n",
        "         decreasing = TRUE)\n",
        "d = data.frame(word = names(v),\n",
        "               freq = v)\n",
        "head(d, 10)\n",
        "```\n",
        "\n",
        " - Step 6: saved as csv for python sentiment analysis. \n",
        "```{r}\n",
        "write.csv(d,\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/countwordR.csv\")\n",
        "```\n",
        "\n",
        " - Step 7: using python and the similar step as the previous data processing to do opinion mining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "twtr = pd.read_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/raw data/twitterR.csv\")\n",
        "\n",
        "# define the function\n",
        "def getSentiments(df):\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    tweet_str = \"\"\n",
        "    tweetscore = []\n",
        "    for tweet in df['text']:\n",
        "        tweet_str = tweet_str + \" \" + tweet\n",
        "        score = sid.polarity_scores(tweet_str)\n",
        "        tweetscore.append(score)\n",
        "    return tweetscore\n",
        "\n",
        "# call the function above to see the result score and form a data frame to record\n",
        "sentiment = getSentiments(twtr)\n",
        "texts = pd.DataFrame(twtr.text)\n",
        "stmscore = pd.DataFrame.from_dict(sentiment)\n",
        "stmscore.head()\n",
        "\n",
        "# relate the text and score for better view\n",
        "txtscore = pd.concat([texts,stmscore],axis=1)\n",
        "print(txtscore.head())\n",
        "\n",
        "# export the csv for future analysis\n",
        "txtscore.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/rstmscore.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('anly-580')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "65b28c80aee1d1db6fcbca19cc6ea5044087ba8fb3f5e0dd14404be4935203a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
