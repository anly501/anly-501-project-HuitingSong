---
title: "Naive Bayes in Python with Labeled Text Data"
pdf-engine: lualatex
format:
  html:
        code-fold: true
        self-contained: true
execute:
    warning: false
---

Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes’ theorem with the “naive” assumption of conditional independence between every pair of features given the value of the class variable. In spite of their apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations, famously document classification and spam filtering. They require a small amount of training data to estimate the necessary parameters. Naive Bayes learners and classifiers can be extremely fast compared to more sophisticated methods. The decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one dimensional distribution. This in turn helps to alleviate problems stemming from the curse of dimensionality.

In this section, I will use the TI-IDF transformer to process my cleaned text data. And then using two different Naive Bayes classifiers for my dataset. Finally, I will conpare model performance through accuracy and confusion matrix to determine which model is better for my data. More of the explaination of my tool using will be introduced in the section they are used. 

## Part 1 - Naive Bayes in labeled text data

### On cleaned datset about twitter users' sentiment of energy and environment.

People's perception of energy consumption is very important. It not only reflects the public's perception, but also indirectly reflects people's concern for environmental issues and their willingness to change the status quo. In this section, I will the dataset that cleaned before with content about twitter user's sentiment about energy and enviornment to do the naive bayes algorithm. The purpose of this section for me is to see how the sentiment analysis goes and can i really get the sentiment about the dataset. and then, this influence my study of knowing how people think about energy and envirnment.

#### Data processing
- Loading the required packages
```{python}
import numpy as np
import pandas as pd
import matplotlib as mlp
import re
from bs4 import BeautifulSoup
import nltk
from nltk.corpus import stopwords
import sklearn
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
import seaborn as sns
```

- Loading the dataset
```{python}
score_r = pd.read_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/rstmscore.csv",index_col=[0])
score_r.head()
```

- Categorize the sentiment score

Since in the data cleaning process. we use sentiment analysis to get the opinion score respect to 'negative','positive','neutral'. Now, here we must make a label for the score for future prediction. The label will be created based on 'compound' variable.

The compound score is the sum of positive, negative & neutral scores which is then normalized between -1(most extreme negative) and +1 (most extreme positive). The more Compound score closer to +1, the higher the positivity of the text.

then, save the labeled sentiment data into cleaned data folder for future reference.

```{python}
conditions = [
    (score_r['compound'] >= 0.33),
    (score_r['compound'] <= -0.33),
    (score_r['compound'] > -0.33) & (score_r['compound'] < 0.33),
]

values = ['positive', 'negative', 'neutral']

score_r["sentiment"] = np.select(conditions, values)

score_r.to_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/cleaned data/stm_r_label.csv")

score_r.head()
```

 - EDA 

From the count plot, we can notice that most of the sentiment in this dataset is negative. Therefore,I can make an asssumption that people alttitude toward different energy and environment is negative, which means that they do not hold good view about energy and enviornment. However, we need to make progress in sentiment analysis model to get the best accuracy for predicting the sentiments.

```{python}
sns.countplot(x=score_r['sentiment'])
plt.title('Sentiment Counts')
```


 - process the text
```{python}
REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]')
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')
STOPWORDS = set(stopwords.words('english'))

def cleanText(text):
    text = BeautifulSoup(text, "lxml").text
    text = text.lower()
    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text
    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text
    text = ' '.join(word for word in text.split() if word not in STOPWORDS) 
    return text

score_r['clean_text'] = score_r['clean_text'].apply(cleanText)
score_r.head()
```

#### Word freqency based on TF-IDF

 - split the dataset into traning set (80%) and test set (20%)。 
```{python}
x_train,x_test,y_train,y_test=train_test_split(score_r['clean_text'],score_r['sentiment'],test_size=0.2,shuffle=True)
```

 - Performs the TF-IDF transformation from a provided matrix of counts.

In the data cleaning section, I use bag of word method to model converts text into fixed-length vectors by counting how many times each word appears. Here, i will use TF-IDF which is Term Frequency Inverse Document Frequency. TFIDF works by proportionally increasing the number of times a word appears in the document but is counterbalanced by the number of documents in which it is present. Hence, words like ‘this’, ’are’ etc., that are commonly present in all the documents are not given a very high rank. 

First, the "TfidfVectorizer" function converts a collection of raw documents into a matrix of TF-IDF features. The stop_words_ attribute can get large and increase the model size when pickling.
```{python}
tfidf = TfidfVectorizer(stop_words= 'english', sublinear_tf= True)
tfidf
```

Second, I fit the x_train data into the TF-IDF and transform them to get the tf-idf scores of a set of sentences. 

I print the result of TF-IDF score matching with tokens, so you can what exactly TF-IDF do here and what it does.

```{python}
tfidf_fitted = tfidf.fit(x_train)
tfidf_scores = tfidf_fitted.transform(x_train)

# to see how the tfidf score the tokens
# get the first vector out for x_train
first_vec=tfidf_scores[0] 
# place tf-idf values in a pandas data frame 
df = pd.DataFrame(first_vec.T.todense(), index=tfidf.get_feature_names(), columns=["tfidf"]) 
df.sort_values(by=["tfidf"],ascending=False)
```


#### Different Naive Bayes model

**1. Bernoulli NB**

Naive Bayes classifier for multivariate Bernoulli models.

 implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but each one is assumed to be a binary-valued (Bernoulli, boolean) variable. Therefore, this class requires samples to be represented as binary-valued feature vectors; if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter).

I use the function from package scikit-learn.
```{python}
bayes = BernoulliNB()
bayes.fit(tfidf_scores, y_train)
```

Train the model to get the accuracy
```{python}
tfidf_scores_test = tfidf_fitted.transform(x_test)
accuracy = bayes.score(tfidf_scores_test,y_test)
print("The accuracy for BernoulliNB is: ",accuracy)
```

In the last step, i get the trained TI-IDF score , so now i can use it to do the prediction. Then, print out the classification report which contain accuracy, f1-score, AUC ROC score.
```{python}
pred = bayes.predict(tfidf_scores_test)
print(classification_report(y_test,pred))
```

From the classification report, the accuracy of Bernoulli NB model is shown as 64% which is not too high but fair. This indicates that this model has 60% chance to correctly predict the sentiment label of input sentences. 


**2. Multinomial NB**

Naive Bayes classifier for multinomial models.
 
 The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.

**Using the model function**
```{python}
nb_clf = MultinomialNB()
nb_clf.fit(tfidf_scores, y_train)
```

**Train the model to get the accuracy**
```{python}
tfidf_scores_test = tfidf_fitted.transform(x_test)
accuracy = nb_clf.score(tfidf_scores_test,y_test)
accuracy
```

In the last step, i get the trained TI-IDF score , so now i can use it to do the prediction. Then, print out the classification report which contain accuracy, f1-score, AUC ROC score.
```{python}
predictions = nb_clf.predict(tfidf_scores_test)
print(classification_report(y_test,predictions))
```

From the classification report, the accuracy of Multinomial NB model is shown as 65% which is not too high but higher than Bernoulli NB model. This indicates that this model has 63% chance to correctly predict the sentiment label of input sentences. 

**Model Comparison**

The confusion matrix for Bernoulli NB
```{python}
cm = confusion_matrix(y_true=y_test, y_pred=pred)

fig, ax = plt.subplots(figsize=(7.5, 7.5))
ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for Bernoulli NB', fontsize=18)
plt.show()
```

The prediction vs true labels graph for Bernoulli NB
```{python}
Bernoulli = pd.DataFrame(pred)
sns.histplot(pred,color="lightblue")
sns.histplot(y_test,color="lightpink")
plt.legend(labels=["predict label","true label"])
plt.title("predicted vs true")
```

The confusion matrix for Multinomial NB
```{python}
cm = confusion_matrix(y_true=y_test, y_pred=predictions)

fig, ax = plt.subplots(figsize=(7.5, 7.5))
ax.matshow(cm, cmap=plt.cm.Blues, alpha=0.3)
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        ax.text(x=j, y=i,s=cm[i, j], va='center', ha='center', size='xx-large')
 
plt.xlabel('Predictions', fontsize=18)
plt.ylabel('Actuals', fontsize=18)
plt.title('Confusion Matrix for Multinomial NB', fontsize=18)
plt.show()
```

The prediction vs true labels graph for Multinormial NB
```{python}
MultinomialNB = pd.DataFrame(predictions)
sns.histplot(predictions,color="coral")
sns.histplot(y_test,color="lightyellow")
plt.legend(labels=["predict label","true label"])
plt.title("predicted vs true")
```

#### Conclusion

From the Confusion matrix, the graphs are kind of weird since it is not like the typical heatmap, but this is because the neutral labels which is '1' are matched in both prediction and true labels. Since the negative sentiment counts for most of the testing set, the confusion matrix will look like that. We can see that both Naive Bayes classifiers predict similar amount of correct labels, but Multinomial NB classifier has higher accuracy in predicting both the negative and positive labels. 

In the previous step, through comparing the accuracy of two model, I think Multinomial NB claasifier is more accuracy in predicting the sentiment label of input sentence since it has 65% accuracy. And through looking at the prediction vs true labels graph, we cannot find any big difference in the number between prediction and true labels from two graphs. Based on the accuracy principle, I will choose Multinomial NB classifier as the model to predict the sentiment labels. However, the accuracy by using Naive Bayes is not really high, so I think in the future study i can try other classification method to make better prediction. 

Based on the Naive Bayes model, I do know 65% of twitter users' alttitude are as the same as the sentiment analysis provided. Inside the sentiment analysis, most of the users are holding negative thinking about different energy and environment.
