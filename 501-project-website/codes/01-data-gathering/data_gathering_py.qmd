---
title: "Data Gathering through Python"
format:
  html:
        code-fold: true
        self-contained: true
execute:
    warning: false
---

Data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. In this section, I will introduction what kind of tools I used, what kind of data I want to get related to my project, and the meaning of the data. Also, the coding process and data result will be displayed. 


### Tools for Data Gathering
 - Twitter API 

 The Twitter API is a set of programmatic endpoints that can be used to understand or build the conversation on Twitter. This API allows you to find and retrieve, engage with, or create a variety of different resources including the following: Tweets, Direct Messages, Spaces, Lists, users, and more. 

 - Resource from U.S. Energy Information Administration 

 The U.S. Energy Information Administration (EIA) is a principal agency of the U.S. Federal Statistical System responsible for collecting, analyzing, and disseminating energy information to promote sound policymaking, efficient markets, and public understanding of energy and its interaction with the economy and the environment. EIA programs cover data on coal, petroleum, natural gas, electric, renewable and nuclear energy. EIA is part of the U.S. Department of Energy. 
 Reference link : https://www.eia.gov/b

### My Project Direction and 10 Questions

 - Directions: 
   1. Analyzing people's perceptions of energy consumption.
   2. Predicting the consumption of different types of energy in the next decade.

 - 10 Questions:
   1. What does increased energy consumption mean for people?
   2. Does clean energy matter for the environmental protection?
   3. Who contributes the most to energy consumption, residence, industry, or commercial?
   4. How do people think about global warming?
   5. Which energy source will be consumed less in the next decade?
   6. What will the overall energy consumption level look like in the next decade?
   7. Will demand for traditional energy sources, such as oil, decline?
   8. Will people choose new energy products? Such as electric cars?
   9. Can new energy replace traditional energy?
   10. Will new energy consumption continue to grow in the next decade?


### Data Type
 - Text Data 

 Text data usually consists of documents which can represent words, sentences or even paragraphs of free flowing text.

 Since I want to study people's attitude towards energy consumption, I want to use the Twitter API to grab the text information. I will set "enviornment" "clean energy" "conventional energy" "energy consumption" as the keywords for the twitter api to get the text data. The data obtained will allow me to analyze people's attitudes towards energy consumption, how people perceive different energy sources, and also to analyze people's attitudes towards protecting the environment in terms of energy. I will also use this text data to analyze the relationship between the environment and energy in the minds of twitter users. I will also keep adjusting the text data later for future research. In data gathering section, I used for loop to search over 600 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. I plan to detect the frequency of words to gain a plot. More than this, I plan to use Naive Bytes to give each tweet a positive or negative attitude.

 - Downloaded Dataset
 
 The data we can collect in our daily life is limited and cannot be guaranteed to be authentic, so we need to rely on the information provided by authorities or data miners to achieve our research needs. I will seek information from authorities, such as U.S. Energy Information Administration (EIA), to filter the datasets that are suitable for my project. According to my research direction and question, I need to collect energy consumption over a recent period of time, which can be collected by the energy consumption of different groups and the consumption of different energy types.

## Data Collection Section

### Twitter API in Python

I will set **"enviornment" "clean energy" "conventional energy" "energy consumption"** as the keywords for the twitter api to get the text data. The data obtained will allow me to analyze people's attitudes towards energy consumption, how people perceive different energy sources, and also to analyze people's attitudes towards protecting the environment in terms of energy. I can also use this text data to analyze the relationship between the environment and energy in the minds of twitter users. I will also keep adjusting the text data later for future research. In data gathering section, I used for loop to search over 800 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. 

- Step 1: The python package we will use in gathering data.
```{python}
import json 
import tweepy
import requests
from pandas import json_normalize 
import pandas as pd
from datetime import datetime
import time
import os
```

- Step 2: Using the keys from Twitter and defined key words to scratch data into json file
```{python}
# READ FILE
f = open("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/codes/01-data-gathering/api-keys.json")
input=json.load(f); 

# LOAD KEYS INTO API
consumer_key=input["consumer_key"]    
consumer_secret=input["consumer_secret"]    
access_token=input["access_token"]    
access_token_secret=input["access_token_secret"]    
bearer_token=input["bearer_token"]

# Set up Connection
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)

# Add the search_twitter function here.
def search_twitter(query, max_results,tweet_fields, bearer_token = bearer_token,lang="en"):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    url = "https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&{}".format(query, max_results,tweet_fields)
    print("--------------",url,"--------------")
    response = requests.request("GET", url, headers=headers)
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

tweet_fields = "tweet.fields=text,author_id,created_at,lang"

data = "/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/raw data/"
search_tweets = ["clean energy","conventional energy","environment","energy consumption"]
for idx,val in enumerate(search_tweets):
    tweets_jsondump = []
    json_response1 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    json_response2 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    json_response3 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    json_response4 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    json_response5 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    json_response6 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    json_response7 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    json_response8 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token,lang="en")
    time.sleep(60)
    for i in json_response1['data']:
        tweets_jsondump.append(i)
    for i in json_response2['data']:
        tweets_jsondump.append(i)
    for i in json_response3['data']:
        tweets_jsondump.append(i)
    for i in json_response4['data']:
        tweets_jsondump.append(i)
    for i in json_response5['data']:
        tweets_jsondump.append(i)
    for i in json_response6['data']:
        tweets_jsondump.append(i)
    for i in json_response7['data']:
        tweets_jsondump.append(i)
    for i in json_response8['data']:
        tweets_jsondump.append(i)
    with open(data+str(val)+'.json','w') as json_file:
        json.dump(tweets_jsondump,json_file)
        json_file.close()
```

- Step 3: Normalize the json file and form a data frame 
```{python}
twitterdf1 = json_normalize(json_response1,"data")
twitterdf2 = json_normalize(json_response2,"data")
twitterdf3 = json_normalize(json_response3,"data")
twitterdf4 = json_normalize(json_response4,"data")
twitterdf5 = json_normalize(json_response5,"data")
twitterdf6 = json_normalize(json_response6,"data")
twitterdf7 = json_normalize(json_response6,"data")
twitterdf8 = json_normalize(json_response6,"data")
twitterdf = [twitterdf1,twitterdf2,twitterdf3,twitterdf4,twitterdf5,twitterdf6,twitterdf7,twitterdf8]
twitterdf = pd.concat(twitterdf)
twitterdf
```

- Step 4: Save the data frame as csv file for futhur using
```{python}
twitterdf.to_csv("/Users/crystal/Desktop/anly-501-project-HuitingSong/501-project-website/data/raw data/twitterpython.csv")
```