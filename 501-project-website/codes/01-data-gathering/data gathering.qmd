---
title: "Data Gathering"
format: 
    html: 
        code-fold: true
        self-contained: true
        css: styles.css
---

Data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. In this section, I will introduction what kind of tools I used, what kind of data I want to get related to my project, and the meaning of the data. Also, the coding process and data result will be displayed. 

### Tools for Data Gathering
 - Twitter API 

 The Twitter API is a set of programmatic endpoints that can be used to understand or build the conversation on Twitter. This API allows you to find and retrieve, engage with, or create a variety of different resources including the following: Tweets, Direct Messages, Spaces, Lists, users, and more. 

 - Resource from U.S. Energy Information Administration <br>

 The U.S. Energy Information Administration (EIA) is a principal agency of the U.S. Federal Statistical System responsible for collecting, analyzing, and disseminating energy information to promote sound policymaking, efficient markets, and public understanding of energy and its interaction with the economy and the environment. EIA programs cover data on coal, petroleum, natural gas, electric, renewable and nuclear energy. EIA is part of the U.S. Department of Energy. 
 Reference link : https://www.eia.gov/


### Data Type
 - Text Data <br>

I used twitter API to scratch mutiple keywords:**"energy" "gas" "electricity" "price" "" "USA"** to gather comments from twiter about their opinion about the energy consumption and energy economy. I use these data to analyze the positive and negative opinion. I can detect the internet users' attitudes to energy influence on economy. I can also use this datasets to define the relationship between economy and data. The keywords may not clear enough for me to analysis but I will adjust it later for future research. I used for loop to search over 600 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. I plan to detect the frequency of words to gain a plot. More than this, I plan to use Naive Bytes to give each tweet a positive or negative attitude.

 - Numerical Data

### Twitter API in Python
- Step 1: The python package we will use in gathering data.
```{python}
import json 
import tweepy
import requests
from pandas import json_normalize 
import pandas as pd
```

- Step 2: Using the keys from Twitter and defined key words to scratch data into json file
```{python}
# READ FILE
f = open("/Users/apple/Desktop/anly-501-project-HuitingSong-1/codes/01-data-gathering/api-keys.json")
input=json.load(f); 

# LOAD KEYS INTO API
consumer_key=input["consumer_key"]    
consumer_secret=input["consumer_secret"]    
access_token=input["access_token"]    
access_token_secret=input["access_token_secret"]    
bearer_token=input["bearer_token"]

# Set up Connection
auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth)

# Add the search_twitter function here.
def search_twitter(query, max_results,tweet_fields, bearer_token = bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    url = "https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&{}".format(query, max_results,tweet_fields)
    print("--------------",url,"--------------")
    response = requests.request("GET", url, headers=headers)
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

tweet_fields = "tweet.fields=text,author_id,created_at,lang"

data = "/Users/apple/Desktop/anly-501-project-HuitingSong-1/data/raw data/"
search_tweets = ["energy","energy economy","consumption","data","USA"]
for idx,val in enumerate(search_tweets):
    tweets_jsondump = []
    json_response1 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response2 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response3 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response4 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response5 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    json_response6 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)
    for i in json_response1['data']:
        tweets_jsondump.append(i)
    for i in json_response2['data']:
        tweets_jsondump.append(i)
    for i in json_response3['data']:
        tweets_jsondump.append(i)
    for i in json_response4['data']:
        tweets_jsondump.append(i)
    for i in json_response5['data']:
        tweets_jsondump.append(i)
    for i in json_response6['data']:
        tweets_jsondump.append(i)
    with open(data+str(val)+'.json','w') as json_file:
        json.dump(tweets_jsondump,json_file)
        json_file.close()
```

- Step 3: Normalize the json file and form a data frame 
```{python}
twitterdf1 = json_normalize(json_response1,"data")
twitterdf2 = json_normalize(json_response2,"data")
twitterdf3 = json_normalize(json_response3,"data")
twitterdf4 = json_normalize(json_response4,"data")
twitterdf5 = json_normalize(json_response5,"data")
twitterdf6 = json_normalize(json_response6,"data")
twitterdf = [twitterdf1,twitterdf2,twitterdf3,twitterdf4,twitterdf5,twitterdf6]
twitterdf = pd.concat(twitterdf)
twitterdf
```

- Step 4: Save the data frame as csv file for futhur using
```{python}
twitterdf.to_csv("/Users/apple/Desktop/anly-501-project-HuitingSong-1/data/raw data/twitterpython.csv")
```

