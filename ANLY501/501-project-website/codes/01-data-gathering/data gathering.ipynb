{
  "cells": [
    {
      "cell_type": "raw",
      "id": "1326e2df",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data Gathering\"\n",
        "format: \n",
        "    html: \n",
        "        code-fold: true\n",
        "        self-contained: true\n",
        "        css: styles.css\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a818c3b",
      "metadata": {},
      "source": [
        "Data gathering is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. In this section, I will introduction what kind of tools I used, what kind of data I want to get related to my project, and the meaning of the data. Also, the coding process and data result will be displayed. \n",
        "\n",
        "### Tools for Data Gathering\n",
        " - Twitter API \n",
        "\n",
        " The Twitter API is a set of programmatic endpoints that can be used to understand or build the conversation on Twitter. This API allows you to find and retrieve, engage with, or create a variety of different resources including the following: Tweets, Direct Messages, Spaces, Lists, users, and more. \n",
        "\n",
        " - Resource from U.S. Energy Information Administration <br>\n",
        "\n",
        " The U.S. Energy Information Administration (EIA) is a principal agency of the U.S. Federal Statistical System responsible for collecting, analyzing, and disseminating energy information to promote sound policymaking, efficient markets, and public understanding of energy and its interaction with the economy and the environment. EIA programs cover data on coal, petroleum, natural gas, electric, renewable and nuclear energy. EIA is part of the U.S. Department of Energy. \n",
        " Reference link : https://www.eia.gov/\n",
        "\n",
        "### My Project Direction and 10 Questions\n",
        "\n",
        " - Directions: \n",
        "   1. Analyzing people's perceptions of energy consumption.\n",
        "   2. Predicting the consumption of different types of energy in the next decade.\n",
        "\n",
        " - 10 Questions:\n",
        "   1. What does increased energy consumption mean for people?\n",
        "   2. Does clean energy matter for the environmental protection?\n",
        "   3. Who contributes the most to energy consumption, residence, industry, or commercial?\n",
        "   4. How do people think about global warming?\n",
        "   5. Which energy source will be consumed less in the next decade?\n",
        "   6. What will the overall energy consumption level look like in the next decade?\n",
        "   7. Will demand for traditional energy sources, such as oil, decline?\n",
        "   8. Will people choose new energy products? Such as electric cars?\n",
        "   9. Can new energy replace traditional energy?\n",
        "   10. Will new energy consumption continue to grow in the next decade?\n",
        "\n",
        "\n",
        "### Data Type\n",
        " - Text Data <br>\n",
        "\n",
        " Text data usually consists of documents which can represent words, sentences or even paragraphs of free flowing text.\n",
        "\n",
        " Since I want to study people's attitude towards energy consumption, I want to use the Twitter API to grab the text information. \n",
        " I will set \"energy consumption\" \"global warming\" \"clean energy\" \"conventional energy\" \"environmental protection\" as the keywords for the twitter api to get the text data. The data obtained will allow me to analyze people's attitudes towards energy consumption, how people perceive different energy sources, and also to analyze people's attitudes towards protecting the environment in terms of energy. I will also use this text data to analyze the relationship between the environment and energy in the minds of twitter users. I will also keep adjusting the text data later for future research. In data gathering section, I used for loop to search over 600 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. I plan to detect the frequency of words to gain a plot. More than this, I plan to use Naive Bytes to give each tweet a positive or negative attitude.\n",
        "\n",
        " - Downloaded Dataset\n",
        " \n",
        " The data we can collect in our daily life is limited and cannot be guaranteed to be authentic, so we need to rely on the information provided by authorities or data miners to achieve our research needs. I will seek information from authorities, such as U.S. Energy Information Administration (EIA), to filter the datasets that are suitable for my project. According to my research direction and question, I need to collect energy consumption over a recent period of time, which can be collected by the energy consumption of different groups and the consumption of different energy types.\n",
        "\n",
        "## Data Collection Section\n",
        "\n",
        "### Twitter API in Python\n",
        "\n",
        "I will set **\"energy consumption\" \"global warming\" \"clean energy\" \"conventional energy\" \"environmental protection\"** as the keywords for the twitter api to get the text data. The data obtained will allow me to analyze people's attitudes towards energy consumption, how people perceive different energy sources, and also to analyze people's attitudes towards protecting the environment in terms of energy. I can also use this text data to analyze the relationship between the environment and energy in the minds of twitter users. I will also keep adjusting the text data later for future research. In data gathering section, I used for loop to search over 600 tweets in order to make comprehensive datasets. I will collect more in the future to scratch over 2000 tweets in order to make sure my results are accurate. \n",
        "\n",
        "- Step 1: The python package we will use in gathering data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91a4f45e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json \n",
        "import tweepy\n",
        "import requests\n",
        "from pandas import json_normalize \n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f1e2705",
      "metadata": {},
      "source": [
        "- Step 2: Using the keys from Twitter and defined key words to scratch data into json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b835516c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=energy consumption&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=energy consumption&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=energy consumption&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=energy consumption&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=energy consumption&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=energy consumption&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=clean energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=clean energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=clean energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=clean energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=clean energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=clean energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=conventional energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=conventional energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=conventional energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=conventional energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=conventional energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=conventional energy&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=environmental protection&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=environmental protection&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=environmental protection&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=environmental protection&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=environmental protection&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=environmental protection&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=global warming&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=global warming&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=global warming&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=global warming&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=global warming&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n",
            "-------------- https://api.twitter.com/2/tweets/search/recent?query=global warming&max_results=100&tweet.fields=text,author_id,created_at,lang --------------\n"
          ]
        }
      ],
      "source": [
        "# READ FILE\n",
        "f = open(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1//501-project-website/codes/01-data-gathering/api-keys.json\")\n",
        "input=json.load(f); \n",
        "\n",
        "# LOAD KEYS INTO API\n",
        "consumer_key=input[\"consumer_key\"]    \n",
        "consumer_secret=input[\"consumer_secret\"]    \n",
        "access_token=input[\"access_token\"]    \n",
        "access_token_secret=input[\"access_token_secret\"]    \n",
        "bearer_token=input[\"bearer_token\"]\n",
        "\n",
        "# Set up Connection\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth)\n",
        "\n",
        "# Add the search_twitter function here.\n",
        "def search_twitter(query, max_results,tweet_fields, bearer_token = bearer_token):\n",
        "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
        "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&max_results={}&{}\".format(query, max_results,tweet_fields)\n",
        "    print(\"--------------\",url,\"--------------\")\n",
        "    response = requests.request(\"GET\", url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise Exception(response.status_code, response.text)\n",
        "    return response.json()\n",
        "\n",
        "tweet_fields = \"tweet.fields=text,author_id,created_at,lang\"\n",
        "\n",
        "data = \"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/raw data/\"\n",
        "search_tweets = [\"energy consumption\",\"clean energy\",\"conventional energy\",\"environmental protection\",\"global warming\"]\n",
        "for idx,val in enumerate(search_tweets):\n",
        "    tweets_jsondump = []\n",
        "    json_response1 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response2 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response3 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response4 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response5 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    json_response6 = search_twitter(query = str(val), max_results = 100, tweet_fields = tweet_fields, bearer_token=bearer_token)\n",
        "    for i in json_response1['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response2['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response3['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response4['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response5['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    for i in json_response6['data']:\n",
        "        tweets_jsondump.append(i)\n",
        "    with open(data+str(val)+'.json','w') as json_file:\n",
        "        json.dump(tweets_jsondump,json_file)\n",
        "        json_file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83c1ef22",
      "metadata": {},
      "source": [
        "- Step 3: Normalize the json file and form a data frame "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2e72c09c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author_id</th>\n",
              "      <th>text</th>\n",
              "      <th>created_at</th>\n",
              "      <th>id</th>\n",
              "      <th>edit_history_tweet_ids</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2506235305</td>\n",
              "      <td>RT @LionHearted76: The Green Agenda is Insane\\...</td>\n",
              "      <td>2022-12-07T03:55:29.000Z</td>\n",
              "      <td>1600338181550796800</td>\n",
              "      <td>[1600338181550796800]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1366635956237926401</td>\n",
              "      <td>@Jerzeebliss129 @TaraBull808 I’m not implying ...</td>\n",
              "      <td>2022-12-07T03:55:23.000Z</td>\n",
              "      <td>1600338156053999621</td>\n",
              "      <td>[1600338156053999621]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>951159840352563201</td>\n",
              "      <td>RT @TomTSEC: Due to global warming, it is curr...</td>\n",
              "      <td>2022-12-07T03:55:19.000Z</td>\n",
              "      <td>1600338138106597377</td>\n",
              "      <td>[1600338138106597377]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1586126676358041600</td>\n",
              "      <td>@JamesMelville It used to be called Global war...</td>\n",
              "      <td>2022-12-07T03:55:16.000Z</td>\n",
              "      <td>1600338123057135620</td>\n",
              "      <td>[1600338123057135620]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1697470002</td>\n",
              "      <td>RT @CoachBalto: Global warming is coming for y...</td>\n",
              "      <td>2022-12-07T03:55:15.000Z</td>\n",
              "      <td>1600338119072808960</td>\n",
              "      <td>[1600338119072808960]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>382939470</td>\n",
              "      <td>@TomTSEC Tell me you know nothing about global...</td>\n",
              "      <td>2022-12-07T03:39:55.000Z</td>\n",
              "      <td>1600334260778729474</td>\n",
              "      <td>[1600334260778729474]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>1528788527709921280</td>\n",
              "      <td>@jordanbpeterson Need some global warming wher...</td>\n",
              "      <td>2022-12-07T03:39:53.000Z</td>\n",
              "      <td>1600334255464194048</td>\n",
              "      <td>[1600334255464194048]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>67601392</td>\n",
              "      <td>RT @BigCityAndrew: \"Shut up and sit down... Gl...</td>\n",
              "      <td>2022-12-07T03:39:44.000Z</td>\n",
              "      <td>1600334215555358720</td>\n",
              "      <td>[1600334215555358720]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>2879969184</td>\n",
              "      <td>RT @UNFCCC: Some good news 📣 from the Internat...</td>\n",
              "      <td>2022-12-07T03:39:35.000Z</td>\n",
              "      <td>1600334176883920899</td>\n",
              "      <td>[1600334176883920899]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>209551769</td>\n",
              "      <td>RT @shelly_187: Global warming!!!! https://t.c...</td>\n",
              "      <td>2022-12-07T03:39:34.000Z</td>\n",
              "      <td>1600334175294341120</td>\n",
              "      <td>[1600334175294341120]</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              author_id                                               text  \\\n",
              "0            2506235305  RT @LionHearted76: The Green Agenda is Insane\\...   \n",
              "1   1366635956237926401  @Jerzeebliss129 @TaraBull808 I’m not implying ...   \n",
              "2    951159840352563201  RT @TomTSEC: Due to global warming, it is curr...   \n",
              "3   1586126676358041600  @JamesMelville It used to be called Global war...   \n",
              "4            1697470002  RT @CoachBalto: Global warming is coming for y...   \n",
              "..                  ...                                                ...   \n",
              "95            382939470  @TomTSEC Tell me you know nothing about global...   \n",
              "96  1528788527709921280  @jordanbpeterson Need some global warming wher...   \n",
              "97             67601392  RT @BigCityAndrew: \"Shut up and sit down... Gl...   \n",
              "98           2879969184  RT @UNFCCC: Some good news 📣 from the Internat...   \n",
              "99            209551769  RT @shelly_187: Global warming!!!! https://t.c...   \n",
              "\n",
              "                  created_at                   id edit_history_tweet_ids lang  \n",
              "0   2022-12-07T03:55:29.000Z  1600338181550796800  [1600338181550796800]   en  \n",
              "1   2022-12-07T03:55:23.000Z  1600338156053999621  [1600338156053999621]   en  \n",
              "2   2022-12-07T03:55:19.000Z  1600338138106597377  [1600338138106597377]   en  \n",
              "3   2022-12-07T03:55:16.000Z  1600338123057135620  [1600338123057135620]   en  \n",
              "4   2022-12-07T03:55:15.000Z  1600338119072808960  [1600338119072808960]   en  \n",
              "..                       ...                  ...                    ...  ...  \n",
              "95  2022-12-07T03:39:55.000Z  1600334260778729474  [1600334260778729474]   en  \n",
              "96  2022-12-07T03:39:53.000Z  1600334255464194048  [1600334255464194048]   en  \n",
              "97  2022-12-07T03:39:44.000Z  1600334215555358720  [1600334215555358720]   en  \n",
              "98  2022-12-07T03:39:35.000Z  1600334176883920899  [1600334176883920899]   en  \n",
              "99  2022-12-07T03:39:34.000Z  1600334175294341120  [1600334175294341120]   en  \n",
              "\n",
              "[600 rows x 6 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "twitterdf1 = json_normalize(json_response1,\"data\")\n",
        "twitterdf2 = json_normalize(json_response2,\"data\")\n",
        "twitterdf3 = json_normalize(json_response3,\"data\")\n",
        "twitterdf4 = json_normalize(json_response4,\"data\")\n",
        "twitterdf5 = json_normalize(json_response5,\"data\")\n",
        "twitterdf6 = json_normalize(json_response6,\"data\")\n",
        "twitterdf = [twitterdf1,twitterdf2,twitterdf3,twitterdf4,twitterdf5,twitterdf6]\n",
        "twitterdf = pd.concat(twitterdf)\n",
        "twitterdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e845ef9c",
      "metadata": {},
      "source": [
        "- Step 4: Save the data frame as csv file for futhur using"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "01e8f396",
      "metadata": {},
      "outputs": [],
      "source": [
        "twitterdf.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1//501-project-website/data/raw data/twitterpython.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.4 ('anly-580')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "65b28c80aee1d1db6fcbca19cc6ea5044087ba8fb3f5e0dd14404be4935203a1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
