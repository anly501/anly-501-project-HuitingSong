{
  "cells": [
    {
      "cell_type": "raw",
      "id": "cbf67f50",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Data Gathering\"\n",
        "pdf-engine: lualatex\n",
        "format:\n",
        "  html:\n",
        "        code-fold: False\n",
        "        self-contained: true\n",
        "execute:\n",
        "    warning: false\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e92907a",
      "metadata": {},
      "source": [
        "**Data cleaning** is the process of fixing or removing incorrect, corrupted, incorrectly formatted, duplicate, or incomplete data within a dataset. When combining multiple data sources, there are many opportunities for data to be duplicated or mislabeled. If data is incorrect, outcomes and algorithms are unreliable, even though they may look correct. There is no one absolute way to prescribe the exact steps in the data cleaning process because the processes will vary from dataset to dataset. But it is crucial to establish a template for your data cleaning process so you know you are doing it the right way every time. In this section, I will clean my text data and modified data. The steps for cleaning these two kind of data is different, so I will specify the cleaning steps with presenting the code. \n",
        "\n",
        "## Python on Text Data \n",
        "\n",
        "#### Dataset \"twitterpython.csv\" which is grabbed through python\n",
        "\n",
        "People's perception of energy consumption is very important. It not only reflects the public's perception, but also indirectly reflects people's concern for environmental issues and their willingness to change the status quo. In the data gathering section, I have gather the text data by setting “energy consumption” “global warming” “clean energy” “conventional energy” “environmental protection” as the keywords for the twitter api to get the text data. Now I will do the data processing for the text dataset through python. \n",
        "\n",
        " - Step 1: import the packages we may use during data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acf24bcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from sklearn import svm\n",
        "import nltk\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21b208a5",
      "metadata": {},
      "source": [
        " - Step 2: Load the Dataset we grab by using python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b108d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "twitter = pd.read_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/raw data/twitterpython.csv\")\n",
        "twitter.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f2d501f",
      "metadata": {},
      "source": [
        " - Step 3: Find the dataset with NA missing value\n",
        " \n",
        " Through checking, there is no missing value in the text dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b07b2a",
      "metadata": {},
      "outputs": [],
      "source": [
        "twitter.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e520ab",
      "metadata": {},
      "source": [
        " - Step 4: Remove data with language other than English\n",
        "\n",
        " Since my Twitter API collect the text with different languages, I need to drop the text information that cannot be understand by me and you. I only keep the text with English."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecfb0f0",
      "metadata": {},
      "outputs": [],
      "source": [
        "twitter = twitter[twitter.lang == \"en\"]\n",
        "twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7252a99f",
      "metadata": {},
      "source": [
        " - Step 5: Pre-processing the text info in the dataset\n",
        " \n",
        " Since the raw data contains lots of puntuations, urls, commas, numbers, highercase, and some other things which will influence our analysis and tokenization. Therefore, we need to remove the unnecessary things. Since this semester, I take Natural Language Processing course, and learn how to use **pipline** to process the text information. So, in this part, I will the method I learn from NLP. \n",
        " \n",
        " Through pipline, the things which will influence our analysis will be cleaned and replaced. And then, we will need to transform the sentence from text data to tokens which is a list of words through tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe88c3c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "pipeline = spacy.load('en_core_web_sm')\n",
        "\n",
        "# http://emailregex.com/\n",
        "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
        "\n",
        "# replace = [ (pattern-to-replace, replacement),  ...]\n",
        "replace = [\n",
        "    (r\"<a[^>]*>(.*?)</a>\", r\"\\1\"),  # Matches most URLs\n",
        "    (email_re, \"email\"),            # Matches emails\n",
        "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
        "    (r\"\\d+\", \"number\"),              # Map digits to special token <numbr>\n",
        "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n",
        "    (r\"\\s+\", \" \")                   # Stips extra whitespace\n",
        "]\n",
        "\n",
        "twitter_sentences = []\n",
        "for i, d in enumerate(twitter['text']):\n",
        "    for repl in replace:\n",
        "        d = re.sub(repl[0], repl[1], d)\n",
        "    twitter_sentences.append(d)\n",
        "\n",
        "@Language.component(\"lab04Preprocessor\")\n",
        "def ng20_preprocess(doc):\n",
        "    tokens = [token for token in doc \n",
        "              if not any((token.is_stop, token.is_punct))]\n",
        "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "pipeline.add_pipe(\"lab04Preprocessor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e99a54",
      "metadata": {},
      "source": [
        " - Step 6: Pass data through spacy pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6765ed0e",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = []\n",
        "for sent in twitter_sentences:\n",
        "    docs.append(pipeline(sent))\n",
        "\n",
        "result = pd.DataFrame(docs)\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed8b7cca",
      "metadata": {},
      "source": [
        " - Step 7: Backup the processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3042fca",
      "metadata": {},
      "outputs": [],
      "source": [
        "result.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/piplineresult.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3abad00b",
      "metadata": {},
      "source": [
        " - Step 8: Using the CountVectorizer and count the words freqency\n",
        " \n",
        " Countvectorizer can convert a collection of text documents to a matrix of token counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fce1c59",
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow = vectorizer.fit(docs)\n",
        "features = bow.vocabulary_.keys()\n",
        "counts = bow.vocabulary_.values()\n",
        "bow=pd.DataFrame({'words':features,'counts':counts})\n",
        "bow = bow.sort_values(by=['counts'],ascending=False)\n",
        "bow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5000823a",
      "metadata": {},
      "source": [
        " - Step 9: Backup the processed data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ee0ac0",
      "metadata": {},
      "outputs": [],
      "source": [
        "bow.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/countword.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff0fec7",
      "metadata": {},
      "source": [
        " - Step 10: Opinion Mining\n",
        " \n",
        " Opinion mining is an approach to natural language processing (NLP) that identifies the emotional tone behind a body of text. This is a popular way for organizations to determine and categorize opinions about a product, service, or idea. In addition to identifying sentiment, opinion mining can extract the polarity (or the amount of positivity and negativity), subject and opinion holder within the text. Furthermore, sentiment analysis can be applied to varying scopes such as document, paragraph, sentence and sub-sentence levels.\n",
        "\n",
        " Since I would like to analysis the attitude people toward the energy consumption, opinion mining is an useful methods to conduct the result. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d29a1cc7",
      "metadata": {
        "md-indent": " "
      },
      "outputs": [],
      "source": [
        "# define the function\n",
        "def getSentiments(df):\n",
        "   sid = SentimentIntensityAnalyzer()\n",
        "   tweet_str = \"\"\n",
        "   tweetscore = []\n",
        "   for tweet in df['text']:\n",
        "       tweet_str = tweet_str + \" \" + tweet\n",
        "       score = sid.polarity_scores(tweet_str)\n",
        "       tweetscore.append(score)\n",
        "   return tweetscore\n",
        "\n",
        "#call the function above to see the result score and form a data frame to record\n",
        "sentiment= getSentiments(twitter)\n",
        "texts= pd.DataFrame(twitter.text)\n",
        "stmscore= pd.DataFrame.from_dict(sentiment)\n",
        "stmscore.head()\n",
        "\n",
        "#relate the text and score for better view\n",
        "txtscore= pd.concat([texts,stmscore],axis=1)\n",
        "txtscore.head()\n",
        "\n",
        "#export the csv for future analysis\n",
        "txtscore.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaneddata/pystmscore.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32047492",
      "metadata": {},
      "source": [
        "#### Dataset \"twitterR.csv\" which is grabbed through python\n",
        "\n",
        "For Twitter API in R, I used twitter to scratch keywords like **\"oil\",\"gas\",\"solar power\",\"wind power\"** to scratch users’ attitudes about these four kinds of energy. I want to search different types of energy in order to analysis people attitude toward different enegy, and what they think about two different kind of energy (renewable energy and convential energy).\n",
        "\n",
        " - Step 1: load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e64965",
      "metadata": {},
      "outputs": [],
      "source": [
        "twtr = pd.read_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/raw data/twitterR.csv\")\n",
        "twtr.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17a029f2",
      "metadata": {},
      "source": [
        " - Step 2: Find the dataset with NA missing value\n",
        " \n",
        " Since R collects lots of basic infor of users and tweet situation, we just need to check whether there are missing value in text variable. Through checking, there is no missing value in the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "869493e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "twtr[\"text\"].isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dd43c4",
      "metadata": {},
      "source": [
        " - Step 3: Pre-processing the text info in the dataset\n",
        " \n",
        " Since the raw data contains lots of puntuations, urls, commas, numbers, highercase, and some other things which will influence our analysis and tokenization. Therefore, we need to remove the unnecessary things. Since this semester, I take Natural Language Processing course, and learn how to use **pipline** to process the text information. So, in this part, I will the method I learn from NLP. \n",
        " \n",
        " Through pipline, the things which will influence our analysis will be cleaned and replaced. And then, we will need to transform the sentence from text data to tokens which is a list of words through tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d93fa9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "from spacy.language import Language\n",
        "\n",
        "pipeline = spacy.load('en_core_web_sm')\n",
        "\n",
        "# http://emailregex.com/\n",
        "email_re = r\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\"\n",
        "\n",
        "# replace = [ (pattern-to-replace, replacement),  ...]\n",
        "replace = [\n",
        "    (r\"<a[^>]*>(.*?)</a>\", r\"\\1\"),  # Matches most URLs\n",
        "    (email_re, \"email\"),            # Matches emails\n",
        "    (r\"(?<=\\d),(?=\\d)\", \"\"),        # Remove commas in numbers\n",
        "    (r\"\\d+\", \"number\"),              # Map digits to special token <numbr>\n",
        "    (r\"[\\t\\n\\r\\*\\.\\@\\,\\-\\/]\", \" \"), # Punctuation and other junk\n",
        "    (r\"\\s+\", \" \")                   # Stips extra whitespace\n",
        "]\n",
        "\n",
        "twitter_sentences = []\n",
        "for i, d in enumerate(twtr['text']):\n",
        "    for repl in replace:\n",
        "        d = re.sub(repl[0], repl[1], d)\n",
        "    twitter_sentences.append(d)\n",
        "\n",
        "@Language.component(\"lab04Preprocessor\")\n",
        "def ng20_preprocess(doc):\n",
        "    tokens = [token for token in doc \n",
        "              if not any((token.is_stop, token.is_punct))]\n",
        "    tokens = [token.lemma_.lower().strip() for token in tokens]\n",
        "    tokens = [token for token in tokens if token]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "pipeline.add_pipe(\"lab04Preprocessor\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3606300c",
      "metadata": {},
      "source": [
        " - Step 6: Pass data through spacy pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4314c3e",
      "metadata": {},
      "outputs": [],
      "source": [
        "docs = []\n",
        "for sent in twitter_sentences:\n",
        "    docs.append(pipeline(sent))\n",
        "\n",
        "result = pd.DataFrame(docs)\n",
        "result.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe155f77",
      "metadata": {},
      "source": [
        " - Step 7: Backup the processed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30e795c5",
      "metadata": {},
      "outputs": [],
      "source": [
        "result.to_csv(\"/Users/apple/Desktop/anly-501-project-HuitingSong-1/501-project-website/data/cleaned data/ppresult.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.15 ('anly580')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "25f0798ac99ef8186442e68090f5f582b27fc47c7bbae071c9dce4da210df78a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
